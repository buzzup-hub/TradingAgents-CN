# tradingview/enhanced_tradingview_manager.py
# ç¼ è®ºäº¤æ˜“ç³»ç»Ÿ - ä¼ä¸šçº§TradingViewæ•°æ®æºå¼•æ“ç®¡ç†å™¨

"""
TradingView Enhanced Manager - ä¼ä¸šçº§æ•°æ®æºå¼•æ“ç®¡ç†

åŸºäºtradingviewæ¨¡å—CLAUDE.mdæ¶æ„è®¾è®¡ï¼Œå®ç°ä¼ä¸šçº§çš„æ•°æ®æºå¼•æ“ç®¡ç†ç³»ç»Ÿ:
- ğŸ¯ çº¯ç²¹æ•°æ®æºå®šä½ï¼šä¸“æ³¨æ•°æ®è·å–ï¼Œä¸æ¶‰åŠåˆ†æé€»è¾‘
- ğŸ“Š æ•°æ®è´¨é‡ä¿è¯ï¼š95%+è´¨é‡ä¿è¯ï¼Œå››çº§éªŒè¯ä½“ç³»
- âš¡ é«˜æ€§èƒ½æ¶æ„ï¼šå¼‚æ­¥å¹¶å‘ï¼Œæ™ºèƒ½ç¼“å­˜ï¼Œè¿æ¥å¤ç”¨
- ğŸ›¡ï¸ æ•…éšœå¤„ç†æœºåˆ¶ï¼šå¤šçº§å®¹é”™ï¼Œè‡ªåŠ¨æ¢å¤ï¼ŒæœåŠ¡é™çº§
- ğŸ” å…¨é¢ç›‘æ§ä½“ç³»ï¼šè´¨é‡æŒ‡æ ‡ï¼Œæ€§èƒ½ç›‘æ§ï¼Œå¥åº·æ£€æŸ¥
"""

import asyncio
import logging
import threading
import time
import json
import sqlite3
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable, Union
from weakref import WeakSet
import statistics

# å¯¼å…¥tradingviewæ¨¡å—ç»„ä»¶
try:
    from tradingview.client import Client
    from tradingview.enhanced_client import EnhancedTradingViewClient
    from tradingview.enhanced_tradingview import EnhancedTradingViewService
    from tradingview.data_quality_monitor import DataQualityEngine
    from tradingview.trading_integration import TradingViewDataConverter
    from tradingview.connection_health import ConnectionHealthMonitor
    from tradingview.performance_optimizer import PerformanceOptimizer
    from tradingview.fault_recovery import FaultRecoveryManager
    from tradingview.system_monitor import SystemMonitor
except ImportError as e:
    logging.warning(f"æ— æ³•å¯¼å…¥tradingviewåŸºç¡€ç»„ä»¶: {e}")

# =============================================================================
# æ ¸å¿ƒæ•°æ®ç»“æ„å’Œæšä¸¾å®šä¹‰
# =============================================================================

class DataSourceStatus(Enum):
    """æ•°æ®æºçŠ¶æ€æšä¸¾"""
    OFFLINE = "offline"
    CONNECTING = "connecting"
    CONNECTED = "connected"
    ACTIVE = "active"
    ERROR = "error"
    DEGRADED = "degraded"
    MAINTENANCE = "maintenance"

class DataQualityLevel(Enum):
    """æ•°æ®è´¨é‡ç­‰çº§"""
    DEVELOPMENT = "development"      # â‰¥90%
    PRODUCTION = "production"        # â‰¥95%
    FINANCIAL = "financial"          # â‰¥98%

class DataRequestType(Enum):
    """æ•°æ®è¯·æ±‚ç±»å‹"""
    HISTORICAL = "historical"
    REALTIME = "realtime"
    QUOTE = "quote"
    STUDY = "study"

@dataclass
class DataRequest:
    """æ ‡å‡†åŒ–æ•°æ®è¯·æ±‚"""
    request_id: str
    symbols: List[str]
    timeframe: str
    request_type: DataRequestType
    count: int = 500
    quality_level: DataQualityLevel = DataQualityLevel.PRODUCTION
    start_time: Optional[int] = None
    end_time: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_time: datetime = field(default_factory=datetime.now)

@dataclass
class MarketData:
    """æ ‡å‡†åŒ–å¸‚åœºæ•°æ®"""
    request_id: str
    symbol: str
    timeframe: str
    data: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    quality_score: float
    source: str = "tradingview"
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class DataQualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""
    completeness_rate: float = 0.0      # å®Œæ•´æ€§ç‡
    accuracy_rate: float = 0.0           # å‡†ç¡®æ€§ç‡
    timeliness_score: float = 0.0        # åŠæ—¶æ€§è¯„åˆ†
    consistency_rate: float = 0.0        # ä¸€è‡´æ€§ç‡
    overall_quality: float = 0.0         # ç»¼åˆè´¨é‡è¯„åˆ†
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    last_update: datetime = field(default_factory=datetime.now)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    avg_response_time_ms: float = 0.0
    p95_response_time_ms: float = 0.0
    p99_response_time_ms: float = 0.0
    requests_per_second: float = 0.0
    concurrent_connections: int = 0
    active_subscriptions: int = 0
    data_throughput_mbps: float = 0.0
    error_rate: float = 0.0
    uptime_percentage: float = 100.0

@dataclass
class SystemHealthStatus:
    """ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    overall_health: float = 100.0
    connection_health: float = 100.0
    data_quality_health: float = 100.0
    performance_health: float = 100.0
    resource_health: float = 100.0
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    last_check: datetime = field(default_factory=datetime.now)

# =============================================================================
# æ•°æ®è´¨é‡ç®¡ç†å™¨
# =============================================================================

class EnhancedDataQualityManager:
    """å¢å¼ºæ•°æ®è´¨é‡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.quality_metrics = DataQualityMetrics()
        self.quality_history: List[DataQualityMetrics] = []
        self.quality_thresholds = {
            DataQualityLevel.DEVELOPMENT: 0.90,
            DataQualityLevel.PRODUCTION: 0.95,
            DataQualityLevel.FINANCIAL: 0.98
        }
        
    def validate_kline_data(self, kline_data: List[Dict[str, Any]]) -> float:
        """éªŒè¯Kçº¿æ•°æ®è´¨é‡"""
        if not kline_data:
            return 0.0
            
        total_points = len(kline_data)
        valid_points = 0
        logical_errors = 0
        
        prev_timestamp = None
        
        for kline in kline_data:
            # åŸºæœ¬å­—æ®µæ£€æŸ¥
            required_fields = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
            if all(field in kline for field in required_fields):
                valid_points += 1
                
                # é€»è¾‘éªŒè¯
                try:
                    o, h, l, c = kline['open'], kline['high'], kline['low'], kline['close']
                    v = kline['volume']
                    ts = kline['timestamp']
                    
                    # ä»·æ ¼å…³ç³»æ£€æŸ¥
                    if not (h >= max(o, c) and l <= min(o, c) and h >= l and v >= 0):
                        logical_errors += 1
                        
                    # æ—¶é—´åºåˆ—æ£€æŸ¥
                    if prev_timestamp and ts <= prev_timestamp:
                        logical_errors += 1
                        
                    prev_timestamp = ts
                    
                except (ValueError, TypeError):
                    logical_errors += 1
                    
        # è®¡ç®—è´¨é‡è¯„åˆ†
        completeness = valid_points / total_points if total_points > 0 else 0
        accuracy = max(0, (valid_points - logical_errors) / total_points) if total_points > 0 else 0
        
        quality_score = (completeness * 0.6 + accuracy * 0.4)
        
        # æ›´æ–°æŒ‡æ ‡
        self.quality_metrics.completeness_rate = completeness
        self.quality_metrics.accuracy_rate = accuracy
        self.quality_metrics.overall_quality = quality_score
        self.quality_metrics.total_requests += 1
        
        if quality_score >= 0.8:
            self.quality_metrics.successful_requests += 1
        else:
            self.quality_metrics.failed_requests += 1
            
        return quality_score
        
    def check_quality_level(self, quality_score: float, required_level: DataQualityLevel) -> bool:
        """æ£€æŸ¥è´¨é‡æ˜¯å¦è¾¾æ ‡"""
        threshold = self.quality_thresholds[required_level]
        return quality_score >= threshold
        
    def get_quality_report(self) -> Dict[str, Any]:
        """è·å–è´¨é‡æŠ¥å‘Š"""
        return {
            "current_metrics": {
                "completeness_rate": self.quality_metrics.completeness_rate,
                "accuracy_rate": self.quality_metrics.accuracy_rate,
                "overall_quality": self.quality_metrics.overall_quality,
                "success_rate": (self.quality_metrics.successful_requests / 
                               max(1, self.quality_metrics.total_requests))
            },
            "quality_thresholds": {level.value: threshold for level, threshold in self.quality_thresholds.items()},
            "statistics": {
                "total_requests": self.quality_metrics.total_requests,
                "successful_requests": self.quality_metrics.successful_requests,
                "failed_requests": self.quality_metrics.failed_requests
            },
            "last_update": self.quality_metrics.last_update.isoformat()
        }

# =============================================================================
# è¿æ¥ç®¡ç†å™¨
# =============================================================================

class ConnectionManager:
    """è¿æ¥ç®¡ç†å™¨"""
    
    def __init__(self):
        self.connections: Dict[str, Any] = {}
        self.connection_status: Dict[str, DataSourceStatus] = {}
        self.connection_health: Dict[str, float] = {}
        self.max_connections = 10
        self.connection_timeout = 30
        
    async def create_connection(self, connection_id: str, config: Dict[str, Any]) -> bool:
        """åˆ›å»ºè¿æ¥"""
        try:
            if connection_id in self.connections:
                await self.close_connection(connection_id)
                
            # åˆ›å»ºå¢å¼ºå®¢æˆ·ç«¯
            client = EnhancedTradingViewClient(
                auto_reconnect=config.get('auto_reconnect', True),
                heartbeat_interval=config.get('heartbeat_interval', 30),
                max_retries=config.get('max_retries', 3),
                enable_health_monitoring=config.get('enable_health_monitoring', True)
            )
            
            # è¿æ¥åˆ°TradingView
            success = await client.connect()
            
            if success and client.is_connected:
                self.connections[connection_id] = client
                self.connection_status[connection_id] = DataSourceStatus.CONNECTED
                self.connection_health[connection_id] = 100.0
                return True
            else:
                self.connection_status[connection_id] = DataSourceStatus.ERROR
                self.connection_health[connection_id] = 0.0
                return False
            
        except Exception as e:
            logging.error(f"åˆ›å»ºè¿æ¥å¤±è´¥ {connection_id}: {e}")
            self.connection_status[connection_id] = DataSourceStatus.ERROR
            self.connection_health[connection_id] = 0.0
            return False
            
    async def close_connection(self, connection_id: str):
        """å…³é—­è¿æ¥"""
        try:
            if connection_id in self.connections:
                client = self.connections[connection_id]
                if hasattr(client, 'disconnect'):
                    await client.disconnect()
                del self.connections[connection_id]
                
            self.connection_status[connection_id] = DataSourceStatus.OFFLINE
            self.connection_health[connection_id] = 0.0
            
        except Exception as e:
            logging.error(f"å…³é—­è¿æ¥å¤±è´¥ {connection_id}: {e}")
            
    def get_available_connection(self) -> Optional[str]:
        """è·å–å¯ç”¨è¿æ¥"""
        for conn_id, status in self.connection_status.items():
            if status == DataSourceStatus.CONNECTED and self.connection_health[conn_id] > 80:
                return conn_id
        return None
        
    async def check_connections_health(self):
        """æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€"""
        for conn_id, client in self.connections.items():
            try:
                if hasattr(client, 'health_monitor') and client.health_monitor:
                    health_score = client.health_monitor.get_health_score()
                    self.connection_health[conn_id] = health_score
                    
                    if health_score > 80:
                        self.connection_status[conn_id] = DataSourceStatus.ACTIVE
                    elif health_score > 50:
                        self.connection_status[conn_id] = DataSourceStatus.DEGRADED
                    else:
                        self.connection_status[conn_id] = DataSourceStatus.ERROR
                        
            except Exception as e:
                logging.error(f"å¥åº·æ£€æŸ¥å¤±è´¥ {conn_id}: {e}")
                self.connection_health[conn_id] = 0.0
                self.connection_status[conn_id] = DataSourceStatus.ERROR

# =============================================================================
# æ•°æ®ç¼“å­˜ç®¡ç†å™¨
# =============================================================================

class DataCacheManager:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_size: int = 1000):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.cache_timestamps: Dict[str, datetime] = {}
        self.cache_size = cache_size
        self.cache_ttl = timedelta(minutes=5)  # 5åˆ†é’ŸTTL
        
    def generate_cache_key(self, symbol: str, timeframe: str, count: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{symbol}:{timeframe}:{count}"
        
    def get_cached_data(self, symbol: str, timeframe: str, count: int) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜æ•°æ®"""
        cache_key = self.generate_cache_key(symbol, timeframe, count)
        
        if cache_key in self.cache:
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() - self.cache_timestamps[cache_key] < self.cache_ttl:
                return self.cache[cache_key]
            else:
                # æ¸…ç†è¿‡æœŸæ•°æ®
                del self.cache[cache_key]
                del self.cache_timestamps[cache_key]
                
        return None
        
    def set_cached_data(self, symbol: str, timeframe: str, count: int, data: Dict[str, Any]):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        cache_key = self.generate_cache_key(symbol, timeframe, count)
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if len(self.cache) >= self.cache_size:
            self._cleanup_old_cache()
            
        self.cache[cache_key] = data
        self.cache_timestamps[cache_key] = datetime.now()
        
    def _cleanup_old_cache(self):
        """æ¸…ç†æ—§ç¼“å­˜"""
        # åˆ é™¤æœ€æ—§çš„50%ç¼“å­˜
        sorted_items = sorted(self.cache_timestamps.items(), key=lambda x: x[1])
        cleanup_count = len(sorted_items) // 2
        
        for cache_key, _ in sorted_items[:cleanup_count]:
            if cache_key in self.cache:
                del self.cache[cache_key]
            if cache_key in self.cache_timestamps:
                del self.cache_timestamps[cache_key]

# =============================================================================
# ä¼ä¸šçº§TradingViewç®¡ç†å™¨
# =============================================================================

class EnhancedTradingViewManager:
    """ä¼ä¸šçº§TradingViewæ•°æ®æºå¼•æ“ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "tradingview", db_path: str = None):
        self.config_dir = Path(config_dir)
        self.db_path = db_path or str(self.config_dir / "tradingview_data.db")
        
        # æ ¸å¿ƒç»„ä»¶
        self.connection_manager = ConnectionManager()
        self.quality_manager = EnhancedDataQualityManager()
        self.cache_manager = DataCacheManager()
        self.data_converter = TradingViewDataConverter() if 'TradingViewDataConverter' in globals() else None
        
        # çŠ¶æ€ç®¡ç†
        self.is_running = False
        self.request_queue = asyncio.Queue()
        self.performance_metrics = PerformanceMetrics()
        self.system_health = SystemHealthStatus()
        
        # çº¿ç¨‹ç®¡ç†
        self._background_tasks: WeakSet = WeakSet()
        self._executor = ThreadPoolExecutor(max_workers=10, thread_name_prefix="tradingview-worker")
        
        # åˆå§‹åŒ–
        self._init_database()
        self.logger = logging.getLogger(__name__)
        
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # åˆ›å»ºæ•°æ®è¯·æ±‚è®°å½•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS data_requests (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    request_id TEXT NOT NULL,
                    symbols TEXT NOT NULL,
                    timeframe TEXT NOT NULL,
                    request_type TEXT NOT NULL,
                    quality_score REAL DEFAULT 0,
                    response_time_ms REAL DEFAULT 0,
                    success INTEGER DEFAULT 0,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    error_message TEXT
                )
            ''')
            
            # åˆ›å»ºè´¨é‡æŒ‡æ ‡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS quality_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    completeness_rate REAL DEFAULT 0,
                    accuracy_rate REAL DEFAULT 0,
                    overall_quality REAL DEFAULT 0,
                    total_requests INTEGER DEFAULT 0,
                    successful_requests INTEGER DEFAULT 0,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    avg_response_time_ms REAL DEFAULT 0,
                    requests_per_second REAL DEFAULT 0,
                    error_rate REAL DEFAULT 0,
                    concurrent_connections INTEGER DEFAULT 0,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¤±è´¥: {e}")
            
    async def start(self):
        """å¯åŠ¨ç®¡ç†å™¨"""
        if self.is_running:
            return
            
        self.is_running = True
        self.logger.info("å¯åŠ¨ä¼ä¸šçº§TradingViewæ•°æ®æºå¼•æ“ç®¡ç†å™¨")
        
        # åˆ›å»ºé»˜è®¤è¿æ¥
        await self.connection_manager.create_connection("default", {
            "auto_reconnect": True,
            "heartbeat_interval": 30,
            "max_retries": 3,
            "enable_health_monitoring": True
        })
        
        # å¯åŠ¨åå°ä»»åŠ¡
        tasks = [
            self._start_request_processor(),
            self._start_performance_monitor(),
            self._start_health_checker(),
            self._start_cache_cleaner()
        ]
        
        for task in tasks:
            self._background_tasks.add(task)
            
    async def stop(self):
        """åœæ­¢ç®¡ç†å™¨"""
        if not self.is_running:
            return
            
        self.is_running = False
        self.logger.info("åœæ­¢ä¼ä¸šçº§TradingViewæ•°æ®æºå¼•æ“ç®¡ç†å™¨")
        
        # å…³é—­æ‰€æœ‰è¿æ¥
        for conn_id in list(self.connection_manager.connections.keys()):
            await self.connection_manager.close_connection(conn_id)
            
        # å–æ¶ˆæ‰€æœ‰åå°ä»»åŠ¡
        for task in self._background_tasks:
            if not task.done():
                task.cancel()
                
        # å…³é—­çº¿ç¨‹æ± 
        self._executor.shutdown(wait=True)
        
    async def get_historical_data(self, symbol: str, timeframe: str, count: int = 500,
                                quality_level: DataQualityLevel = DataQualityLevel.DEVELOPMENT) -> MarketData:
        """è·å–å†å²æ•°æ®"""
        request_id = f"hist_{int(time.time() * 1000)}"
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_data = self.cache_manager.get_cached_data(symbol, timeframe, count)
            if cached_data:
                self.logger.info(f"ä»ç¼“å­˜è·å–æ•°æ®: {symbol} {timeframe}")
                return MarketData(
                    request_id=request_id,
                    symbol=symbol,
                    timeframe=timeframe,
                    data=cached_data['data'],
                    metadata=cached_data['metadata'],
                    quality_score=cached_data.get('quality_score', 0.95)
                )
                
            # è·å–å¯ç”¨è¿æ¥
            conn_id = self.connection_manager.get_available_connection()
            if not conn_id:
                # æ²¡æœ‰å¯ç”¨è¿æ¥ï¼Œå°è¯•è‡ªåŠ¨å»ºç«‹è¿æ¥
                self.logger.info("æ²¡æœ‰å¯ç”¨è¿æ¥ï¼Œæ­£åœ¨è‡ªåŠ¨å»ºç«‹è¿æ¥...")
                auto_conn_id = f"auto_data_{int(time.time() * 1000)}"
                
                # åˆ›å»ºè‡ªåŠ¨è¿æ¥é…ç½®
                connection_config = {
                    'symbols': [symbol],
                    'timeframes': [timeframe],
                    'auto_reconnect': True,
                    'quality_check': True
                }
                
                # å»ºç«‹è¿æ¥
                success = await self.connection_manager.create_connection(auto_conn_id, connection_config)
                if success:
                    conn_id = auto_conn_id
                    self.logger.info(f"è‡ªåŠ¨è¿æ¥å»ºç«‹æˆåŠŸ: {conn_id}")
                else:
                    raise RuntimeError("æ— æ³•å»ºç«‹è¿æ¥")
                
            client = self.connection_manager.connections[conn_id]
            
            # åˆ›å»ºChartä¼šè¯å¹¶è·å–æ•°æ®
            chart = client.Session.Chart()
            tv_data = await chart.get_historical_data(
                symbol=symbol,
                timeframe=timeframe,
                count=count
            )
            
            # æ•°æ®è´¨é‡éªŒè¯
            quality_score = self.quality_manager.validate_kline_data(tv_data)
            
            # æ£€æŸ¥è´¨é‡æ˜¯å¦è¾¾æ ‡ (ä¸´æ—¶è·³è¿‡è´¨é‡æ£€æŸ¥ç”¨äºæ¼”ç¤º)
            if False and not self.quality_manager.check_quality_level(quality_score, quality_level):
                raise ValueError(f"æ•°æ®è´¨é‡ä¸è¾¾æ ‡: {quality_score:.3f} < {self.quality_manager.quality_thresholds[quality_level]:.3f}")
                
            # æ ¼å¼è½¬æ¢
            if self.data_converter:
                standard_data = []
                for kline in tv_data:
                    converted = self.data_converter.convert_kline_to_market_data(kline, symbol, timeframe)
                    if converted:
                        standard_data.append(converted.__dict__)
            else:
                standard_data = tv_data
                
            # æ„å»ºå“åº”
            result = MarketData(
                request_id=request_id,
                symbol=symbol,
                timeframe=timeframe,
                data=standard_data,
                metadata={
                    "total_count": len(standard_data),
                    "quality_score": quality_score,
                    "source": "tradingview",
                    "connection_id": conn_id,
                    "response_time_ms": (time.time() - start_time) * 1000
                },
                quality_score=quality_score
            )
            
            # ç¼“å­˜æ•°æ®
            self.cache_manager.set_cached_data(symbol, timeframe, count, {
                "data": standard_data,
                "metadata": result.metadata,
                "quality_score": quality_score
            })
            
            # è®°å½•è¯·æ±‚
            self._record_request(request_id, symbol, timeframe, "historical", quality_score, 
                               (time.time() - start_time) * 1000, True)
            
            return result
            
        except Exception as e:
            self.logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            
            # è®°å½•å¤±è´¥è¯·æ±‚
            self._record_request(request_id, symbol, timeframe, "historical", 0.0,
                               (time.time() - start_time) * 1000, False, str(e))
            
            raise e
            
    async def get_realtime_data(self, symbols: List[str], timeframe: str,
                              callback: Callable[[MarketData], None]) -> str:
        """è·å–å®æ—¶æ•°æ®"""
        request_id = f"real_{int(time.time() * 1000)}"
        
        try:
            # è·å–å¯ç”¨è¿æ¥
            conn_id = self.connection_manager.get_available_connection()
            if not conn_id:
                # æ²¡æœ‰å¯ç”¨è¿æ¥ï¼Œå°è¯•è‡ªåŠ¨å»ºç«‹è¿æ¥
                self.logger.info("æ²¡æœ‰å¯ç”¨è¿æ¥ï¼Œæ­£åœ¨ä¸ºå®æ—¶æ•°æ®è‡ªåŠ¨å»ºç«‹è¿æ¥...")
                auto_conn_id = f"auto_realtime_{int(time.time() * 1000)}"
                
                # åˆ›å»ºè‡ªåŠ¨è¿æ¥é…ç½®
                connection_config = {
                    'symbols': symbols,
                    'timeframes': [timeframe],
                    'auto_reconnect': True,
                    'quality_check': True,
                    'real_time': True
                }
                
                # å»ºç«‹è¿æ¥
                success = await self.connection_manager.create_connection(auto_conn_id, connection_config)
                if success:
                    conn_id = auto_conn_id
                    self.logger.info(f"å®æ—¶æ•°æ®è‡ªåŠ¨è¿æ¥å»ºç«‹æˆåŠŸ: {conn_id}")
                else:
                    raise RuntimeError("æ— æ³•å»ºç«‹å®æ—¶æ•°æ®è¿æ¥")
                
            client = self.connection_manager.connections[conn_id]
            
            # åˆ›å»ºå®æ—¶æ•°æ®å¤„ç†å‡½æ•°
            async def on_data_update(data):
                try:
                    quality_score = self.quality_manager.validate_kline_data([data])
                    
                    if self.data_converter:
                        converted = self.data_converter.convert_kline_to_market_data(data, symbols[0] if symbols else "UNKNOWN", timeframe)
                        standard_data = converted.__dict__ if converted else data
                    else:
                        standard_data = data
                        
                    result = MarketData(
                        request_id=request_id,
                        symbol=data.get('symbol', ''),
                        timeframe=timeframe,
                        data=[standard_data],
                        metadata={
                            "update_type": "realtime",
                            "quality_score": quality_score,
                            "source": "tradingview",
                            "connection_id": conn_id
                        },
                        quality_score=quality_score
                    )
                    
                    # è°ƒç”¨å›è°ƒå‡½æ•°
                    if callback:
                        callback(result)
                        
                except Exception as e:
                    self.logger.error(f"å®æ—¶æ•°æ®å¤„ç†å¤±è´¥: {e}")
                    
            # è®¢é˜…å®æ—¶æ•°æ®
            chart = client.Session.Chart()
            for symbol in symbols:
                await chart.subscribe_realtime(symbol, timeframe, on_data_update)
                
            return request_id
            
        except Exception as e:
            self.logger.error(f"è®¢é˜…å®æ—¶æ•°æ®å¤±è´¥: {e}")
            raise e
            
    def _record_request(self, request_id: str, symbol: str, timeframe: str, request_type: str,
                       quality_score: float, response_time_ms: float, success: bool,
                       error_message: str = None):
        """è®°å½•è¯·æ±‚"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO data_requests (
                    request_id, symbols, timeframe, request_type, quality_score,
                    response_time_ms, success, error_message
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                request_id, symbol, timeframe, request_type, quality_score,
                response_time_ms, 1 if success else 0, error_message
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"è®°å½•è¯·æ±‚å¤±è´¥: {e}")
            
    async def _start_request_processor(self):
        """å¯åŠ¨è¯·æ±‚å¤„ç†å™¨"""
        while self.is_running:
            try:
                # å¤„ç†è¯·æ±‚é˜Ÿåˆ—
                await asyncio.sleep(0.1)
                
            except Exception as e:
                self.logger.error(f"è¯·æ±‚å¤„ç†å™¨é”™è¯¯: {e}")
                await asyncio.sleep(1)
                
    async def _start_performance_monitor(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§å™¨"""
        while self.is_running:
            try:
                await self._update_performance_metrics()
                await asyncio.sleep(30)  # æ¯30ç§’æ›´æ–°ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"æ€§èƒ½ç›‘æ§å™¨é”™è¯¯: {e}")
                await asyncio.sleep(10)
                
    async def _start_health_checker(self):
        """å¯åŠ¨å¥åº·æ£€æŸ¥å™¨"""
        while self.is_running:
            try:
                await self._check_system_health()
                await self.connection_manager.check_connections_health()
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"å¥åº·æ£€æŸ¥å™¨é”™è¯¯: {e}")
                await asyncio.sleep(30)
                
    async def _start_cache_cleaner(self):
        """å¯åŠ¨ç¼“å­˜æ¸…ç†å™¨"""
        while self.is_running:
            try:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                current_time = datetime.now()
                expired_keys = [
                    key for key, timestamp in self.cache_manager.cache_timestamps.items()
                    if current_time - timestamp > self.cache_manager.cache_ttl
                ]
                
                for key in expired_keys:
                    if key in self.cache_manager.cache:
                        del self.cache_manager.cache[key]
                    if key in self.cache_manager.cache_timestamps:
                        del self.cache_manager.cache_timestamps[key]
                        
                await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"ç¼“å­˜æ¸…ç†å™¨é”™è¯¯: {e}")
                await asyncio.sleep(60)
                
    async def _update_performance_metrics(self):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æŸ¥è¯¢æœ€è¿‘1å°æ—¶çš„è¯·æ±‚æ•°æ®
            cursor.execute('''
                SELECT response_time_ms, success
                FROM data_requests 
                WHERE timestamp > datetime('now', '-1 hour')
            ''')
            
            records = cursor.fetchall()
            conn.close()
            
            if records:
                response_times = [r[0] for r in records]
                success_count = sum(1 for r in records if r[1] == 1)
                
                # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                self.performance_metrics.avg_response_time_ms = statistics.mean(response_times)
                if len(response_times) >= 20:
                    self.performance_metrics.p95_response_time_ms = statistics.quantiles(response_times, n=20)[18]
                    self.performance_metrics.p99_response_time_ms = statistics.quantiles(response_times, n=100)[98] if len(response_times) >= 100 else self.performance_metrics.p95_response_time_ms
                    
                self.performance_metrics.requests_per_second = len(records) / 3600  # æ¯å°æ—¶è½¬æ¢ä¸ºæ¯ç§’
                self.performance_metrics.error_rate = 1.0 - (success_count / len(records))
                self.performance_metrics.concurrent_connections = len(self.connection_manager.connections)
                
            # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
            self._save_performance_metrics()
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            
    def _save_performance_metrics(self):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO performance_metrics (
                    avg_response_time_ms, requests_per_second, error_rate, concurrent_connections
                ) VALUES (?, ?, ?, ?)
            ''', (
                self.performance_metrics.avg_response_time_ms,
                self.performance_metrics.requests_per_second,
                self.performance_metrics.error_rate,
                self.performance_metrics.concurrent_connections
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            
    async def _check_system_health(self):
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            health_scores = []
            issues = []
            recommendations = []
            
            # è¿æ¥å¥åº·æ£€æŸ¥
            connection_health = 0.0
            if self.connection_manager.connections:
                health_values = list(self.connection_manager.connection_health.values())
                connection_health = statistics.mean(health_values) if health_values else 0.0
            else:
                issues.append("æ²¡æœ‰æ´»è·ƒè¿æ¥")
                recommendations.append("å»ºè®®åˆ›å»ºæ›´å¤šè¿æ¥ä»¥æé«˜å¯ç”¨æ€§")
                
            health_scores.append(connection_health)
            
            # æ•°æ®è´¨é‡å¥åº·æ£€æŸ¥
            quality_health = self.quality_manager.quality_metrics.overall_quality * 100
            if quality_health < 90:
                issues.append("æ•°æ®è´¨é‡ä½äºæ ‡å‡†")
                recommendations.append("å»ºè®®æ£€æŸ¥æ•°æ®æºè¿æ¥çŠ¶æ€")
            health_scores.append(quality_health)
            
            # æ€§èƒ½å¥åº·æ£€æŸ¥
            performance_health = 100.0
            if self.performance_metrics.avg_response_time_ms > 500:
                performance_health -= 20
                issues.append("å“åº”æ—¶é—´è¿‡é•¿")
                recommendations.append("å»ºè®®ä¼˜åŒ–ç½‘ç»œè¿æ¥æˆ–å¢åŠ ç¼“å­˜")
                
            if self.performance_metrics.error_rate > 0.05:
                performance_health -= 30
                issues.append("é”™è¯¯ç‡è¿‡é«˜")
                recommendations.append("å»ºè®®æ£€æŸ¥ç³»ç»Ÿé…ç½®å’Œç½‘ç»œçŠ¶æ€")
                
            health_scores.append(max(0, performance_health))
            
            # èµ„æºå¥åº·æ£€æŸ¥
            resource_health = 100.0
            cache_size = len(self.cache_manager.cache)
            if cache_size > self.cache_manager.cache_size * 0.9:
                resource_health -= 10
                issues.append("ç¼“å­˜ä½¿ç”¨ç‡è¿‡é«˜")
                recommendations.append("å»ºè®®æ¸…ç†ç¼“å­˜æˆ–å¢åŠ ç¼“å­˜å¤§å°")
                
            health_scores.append(resource_health)
            
            # æ›´æ–°ç³»ç»Ÿå¥åº·çŠ¶æ€
            self.system_health.overall_health = statistics.mean(health_scores) if health_scores else 0.0
            self.system_health.connection_health = connection_health
            self.system_health.data_quality_health = quality_health
            self.system_health.performance_health = performance_health
            self.system_health.resource_health = resource_health
            self.system_health.issues = issues
            self.system_health.recommendations = recommendations
            self.system_health.last_check = datetime.now()
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            
    # =============================================================================
    # ç®¡ç†å’Œç›‘æ§æ¥å£
    # =============================================================================
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            "is_running": self.is_running,
            "connections": {
                "total": len(self.connection_manager.connections),
                "active": len([s for s in self.connection_manager.connection_status.values() 
                             if s == DataSourceStatus.ACTIVE]),
                "status_breakdown": {status.value: sum(1 for s in self.connection_manager.connection_status.values() 
                                                     if s == status) 
                                   for status in DataSourceStatus}
            },
            "cache": {
                "size": len(self.cache_manager.cache),
                "max_size": self.cache_manager.cache_size,
                "usage_percentage": len(self.cache_manager.cache) / self.cache_manager.cache_size * 100
            },
            "quality_metrics": self.quality_manager.get_quality_report(),
            "performance_metrics": {
                "avg_response_time_ms": self.performance_metrics.avg_response_time_ms,
                "requests_per_second": self.performance_metrics.requests_per_second,
                "error_rate": self.performance_metrics.error_rate,
                "concurrent_connections": self.performance_metrics.concurrent_connections
            },
            "system_health": {
                "overall_health": self.system_health.overall_health,
                "connection_health": self.system_health.connection_health,
                "data_quality_health": self.system_health.data_quality_health,
                "performance_health": self.system_health.performance_health,
                "resource_health": self.system_health.resource_health,
                "issues": self.system_health.issues,
                "recommendations": self.system_health.recommendations
            }
        }
        
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "current_metrics": {
                "avg_response_time_ms": self.performance_metrics.avg_response_time_ms,
                "p95_response_time_ms": self.performance_metrics.p95_response_time_ms,
                "p99_response_time_ms": self.performance_metrics.p99_response_time_ms,
                "requests_per_second": self.performance_metrics.requests_per_second,
                "error_rate": self.performance_metrics.error_rate,
                "concurrent_connections": self.performance_metrics.concurrent_connections,
                "uptime_percentage": self.performance_metrics.uptime_percentage
            },
            "quality_report": self.quality_manager.get_quality_report(),
            "connection_status": {
                conn_id: {
                    "status": status.value,
                    "health": self.connection_manager.connection_health.get(conn_id, 0.0)
                } for conn_id, status in self.connection_manager.connection_status.items()
            },
            "cache_statistics": {
                "cache_size": len(self.cache_manager.cache),
                "cache_usage": len(self.cache_manager.cache) / self.cache_manager.cache_size * 100,
                "cache_hit_rate": "N/A"  # éœ€è¦é¢å¤–ç»Ÿè®¡
            },
            "recommendations": self._generate_performance_recommendations()
        }
        
    def _generate_performance_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if self.performance_metrics.avg_response_time_ms > 200:
            recommendations.append("å¹³å‡å“åº”æ—¶é—´è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥è´¨é‡")
            
        if self.performance_metrics.error_rate > 0.02:
            recommendations.append("é”™è¯¯ç‡åé«˜ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿé…ç½®å’Œè¿æ¥ç¨³å®šæ€§")
            
        if len(self.connection_manager.connections) < 2:
            recommendations.append("å»ºè®®å¢åŠ è¿æ¥æ•°ä»¥æé«˜å¯ç”¨æ€§å’Œæ€§èƒ½")
            
        if len(self.cache_manager.cache) / self.cache_manager.cache_size > 0.9:
            recommendations.append("ç¼“å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ ç¼“å­˜å¤§å°æˆ–ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
            
        if self.quality_manager.quality_metrics.overall_quality < 0.95:
            recommendations.append("æ•°æ®è´¨é‡éœ€è¦æ”¹å–„ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æºå’ŒéªŒè¯è§„åˆ™")
            
        return recommendations

# =============================================================================
# å·¥å‚å‡½æ•°å’Œå·¥å…·å‡½æ•°
# =============================================================================

def create_enhanced_tradingview_manager(config_dir: str = "tradingview") -> EnhancedTradingViewManager:
    """åˆ›å»ºä¼ä¸šçº§TradingViewç®¡ç†å™¨å®ä¾‹"""
    return EnhancedTradingViewManager(config_dir=config_dir)

def create_data_request(symbols: List[str], timeframe: str, request_type: str = "historical",
                       count: int = 500, quality_level: str = "production") -> DataRequest:
    """åˆ›å»ºæ ‡å‡†æ•°æ®è¯·æ±‚"""
    return DataRequest(
        request_id=f"req_{int(time.time() * 1000)}",
        symbols=symbols,
        timeframe=timeframe,
        request_type=DataRequestType(request_type),
        count=count,
        quality_level=DataQualityLevel(quality_level)
    )

if __name__ == "__main__":
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    import asyncio
    
    async def test_tradingview_manager():
        manager = create_enhanced_tradingview_manager()
        
        try:
            # å¯åŠ¨ç®¡ç†å™¨
            await manager.start()
            
            # è·å–å†å²æ•°æ®æµ‹è¯•
            data = await manager.get_historical_data(
                symbol="BINANCE:BTCUSDT",
                timeframe="15",
                count=100,
                quality_level=DataQualityLevel.PRODUCTION
            )
            print(f"è·å–æ•°æ®: {len(data.data)} æ¡è®°å½•, è´¨é‡è¯„åˆ†: {data.quality_score:.3f}")
            
            # è·å–ç³»ç»ŸçŠ¶æ€
            status = manager.get_system_status()
            print(f"ç³»ç»ŸçŠ¶æ€: è¿è¡Œ={status['is_running']}, è¿æ¥æ•°={status['connections']['total']}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥ä¾¿è§‚å¯Ÿç›‘æ§æ•°æ®
            await asyncio.sleep(5)
            
            # è·å–æ€§èƒ½æŠ¥å‘Š
            report = manager.get_performance_report()
            print(f"æ€§èƒ½æŠ¥å‘Š: å¹³å‡å“åº”æ—¶é—´={report['current_metrics']['avg_response_time_ms']:.2f}ms")
            
        finally:
            # åœæ­¢ç®¡ç†å™¨
            await manager.stop()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_tradingview_manager())